{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#DATA-SCIENCE-WORKFLOW\" data-toc-modified-id=\"DATA-SCIENCE-WORKFLOW-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>DATA SCIENCE WORKFLOW</a></div><div class=\"lev3 toc-item\"><a href=\"#LEARNING-OBJECTIVES\" data-toc-modified-id=\"LEARNING-OBJECTIVES-101\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>LEARNING OBJECTIVES</a></div><div class=\"lev3 toc-item\"><a href=\"#Introduction-(5-mins)\" data-toc-modified-id=\"Introduction-(5-mins)-102\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Introduction (5 mins)</a></div><div class=\"lev3 toc-item\"><a href=\"#Guided-practice-(10-mins)\" data-toc-modified-id=\"Guided-practice-(10-mins)-103\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>Guided practice (10 mins)</a></div><div class=\"lev2 toc-item\"><a href=\"#CRISP-DM-(Cross-Industry-Standard-Process-for-Data-Mining)\" data-toc-modified-id=\"CRISP-DM-(Cross-Industry-Standard-Process-for-Data-Mining)-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>CRISP DM (Cross Industry Standard Process for Data Mining)</a></div><div class=\"lev2 toc-item\"><a href=\"#Data-Science-Project-Lifecycle\" data-toc-modified-id=\"Data-Science-Project-Lifecycle-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data Science Project Lifecycle</a></div><div class=\"lev2 toc-item\"><a href=\"#Data-Science-Workflow\" data-toc-modified-id=\"Data-Science-Workflow-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Data Science Workflow</a></div><div class=\"lev3 toc-item\"><a href=\"#Preparation\" data-toc-modified-id=\"Preparation-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Preparation</a></div><div class=\"lev4 toc-item\"><a href=\"#1.-IDENTIFY-THE-PROBLEM-(ASK-AN-INTERESTING-(GOOD/-WORTH-ANSWERING)-QUESTION)\" data-toc-modified-id=\"1.-IDENTIFY-THE-PROBLEM-(ASK-AN-INTERESTING-(GOOD/-WORTH-ANSWERING)-QUESTION)-1311\"><span class=\"toc-item-num\">1.3.1.1&nbsp;&nbsp;</span>1. IDENTIFY THE PROBLEM (ASK AN INTERESTING (GOOD/ WORTH ANSWERING) QUESTION)</a></div><div class=\"lev4 toc-item\"><a href=\"#2.-ACQUIRE-THE-DATA-(GET-THE-DATA)\" data-toc-modified-id=\"2.-ACQUIRE-THE-DATA-(GET-THE-DATA)-1312\"><span class=\"toc-item-num\">1.3.1.2&nbsp;&nbsp;</span>2. ACQUIRE THE DATA (GET THE DATA)</a></div><div class=\"lev4 toc-item\"><a href=\"#3.-PARSE-THE-DATA\" data-toc-modified-id=\"3.-PARSE-THE-DATA-1313\"><span class=\"toc-item-num\">1.3.1.3&nbsp;&nbsp;</span>3. PARSE THE DATA</a></div><div class=\"lev4 toc-item\"><a href=\"#4.-MINE-THE-DATA\" data-toc-modified-id=\"4.-MINE-THE-DATA-1314\"><span class=\"toc-item-num\">1.3.1.4&nbsp;&nbsp;</span>4. MINE THE DATA</a></div><div class=\"lev3 toc-item\"><a href=\"#Analysis\" data-toc-modified-id=\"Analysis-132\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Analysis</a></div><div class=\"lev4 toc-item\"><a href=\"#5.-REFINE-THE-DATA--(EXPLORE-THE-DATA)\" data-toc-modified-id=\"5.-REFINE-THE-DATA--(EXPLORE-THE-DATA)-1321\"><span class=\"toc-item-num\">1.3.2.1&nbsp;&nbsp;</span>5. REFINE THE DATA  (EXPLORE THE DATA)</a></div><div class=\"lev4 toc-item\"><a href=\"#6.-BUILD-A-DATA-MODEL\" data-toc-modified-id=\"6.-BUILD-A-DATA-MODEL-1322\"><span class=\"toc-item-num\">1.3.2.2&nbsp;&nbsp;</span>6. BUILD A DATA MODEL</a></div><div class=\"lev3 toc-item\"><a href=\"#REFLECTION\" data-toc-modified-id=\"REFLECTION-133\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>REFLECTION</a></div><div class=\"lev4 toc-item\"><a href=\"#7.-PRESENT-RESULTS-(COMMUNICATE-THE-DATA)\" data-toc-modified-id=\"7.-PRESENT-RESULTS-(COMMUNICATE-THE-DATA)-1331\"><span class=\"toc-item-num\">1.3.3.1&nbsp;&nbsp;</span>7. PRESENT RESULTS (COMMUNICATE THE DATA)</a></div><div class=\"lev4 toc-item\"><a href=\"#8.-DEPLOY-AND-VALIDATE-(IMPLEMENTATION)\" data-toc-modified-id=\"8.-DEPLOY-AND-VALIDATE-(IMPLEMENTATION)-1332\"><span class=\"toc-item-num\">1.3.3.2&nbsp;&nbsp;</span>8. DEPLOY AND VALIDATE (IMPLEMENTATION)</a></div><div class=\"lev4 toc-item\"><a href=\"#9.-TEST-AND-QUANTIFY-YOUR-IMPACT\" data-toc-modified-id=\"9.-TEST-AND-QUANTIFY-YOUR-IMPACT-1333\"><span class=\"toc-item-num\">1.3.3.3&nbsp;&nbsp;</span>9. TEST AND QUANTIFY YOUR IMPACT</a></div><div class=\"lev3 toc-item\"><a href=\"#DISSEMINATION\" data-toc-modified-id=\"DISSEMINATION-134\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>DISSEMINATION</a></div><div class=\"lev2 toc-item\"><a href=\"#Independent-practice-(5-mins)\" data-toc-modified-id=\"Independent-practice-(5-mins)-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Independent practice (5 mins)</a></div><div class=\"lev2 toc-item\"><a href=\"#ADDITIONAL-RESOURCES\" data-toc-modified-id=\"ADDITIONAL-RESOURCES-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>ADDITIONAL RESOURCES</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) \n",
    "\n",
    "# DATA SCIENCE WORKFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Describe and list data science workflow steps\n",
    "- Find a workflow that best fits a data science project\n",
    "- Learn steps for data science workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction (5 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Science Workflow is a Non- Linear and Iterative Process.\n",
    "\n",
    "Many of the steps involve other parites and varying parts of the business. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with data, it helps to have a well defined workflow. Specifically, whether we want to perform an analysis with the sole intent of \"telling the story\" (Data Visualisation/Journalism) or build a system that relies on data to model a certain task (Data Mining), process matters. By defining a methodology in advance, teams are in sync and it is easier to avoid losing time trying to figure out what the next step should be. This enables a faster production of results and publication of materials.\n",
    "\n",
    "[Source](http://blog.binaryedge.io/2015/09/08/the-data-science-workflow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science should produce some usable results, and those results can be anything from a list of recommendations to a dashboard to a single chart or any other product that aides in making a more informed decision. The process used to created those data products needs a bit more formalization. Call it a: methodology, process, lifecycle, or workflow; but it needs to exist.\n",
    "\n",
    "[Source](http://101.datascience.community/tag/workflow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img  src=\"DSI-workflow-v1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://cacm.acm.org/system/assets/0001/3678/rp-overview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided practice (10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP DM (Cross Industry Standard Process for Data Mining)\n",
    "\n",
    "Oldest and Most Popular Method for Data Mining\n",
    "\n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Data Preparation\n",
    "4. Modeling\n",
    "5. Evaluation\n",
    "6. Deployment\n",
    "\n",
    "https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Project Lifecycle\n",
    "\n",
    "Recent modification/improvement of CRISP-DM with a bit more of an engineering focus.\n",
    "\n",
    "1. Data acquisition\n",
    "2. Data preparation\n",
    "3. Hypothesis and modeling\n",
    "4. Evaluation and Interpretation\n",
    "5. Deployment\n",
    "6. Operations\n",
    "7. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1. IDENTIFY THE PROBLEM (ASK AN INTERESTING (GOOD/ WORTH ANSWERING) QUESTION)\n",
    "\n",
    "- Identify business/product objectives\n",
    "- Identify and hypothesize goals and criteria for success\n",
    "- Create a set of questions for identify correct data set\n",
    "\n",
    ">Skills: science, domain expertise, curiosity, business and product knowledge\n",
    "\n",
    ">Tools: your brain, talking to experts (in and outside your co.), experience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ACQUIRE THE DATA (GET THE DATA)\n",
    "\n",
    "- Identify the \"right\" data set(s)\n",
    "- Import data and set up local or remote data structure\n",
    "- Determine most appropriate tools to work with data\n",
    "\n",
    ">Skills: data cleaning, querying databases, web scraping, CS stuff\n",
    "\n",
    ">Tools: SQL, python, pandas, (spark)\n",
    "\n",
    "Acquire data:  \n",
    "\n",
    "Data can be acquired from a variety of sources.\n",
    "\n",
    " > Data files can be downloaded from online repositories such as public websites (e.g., U.S. Census data sets).\n",
    "\n",
    "> Data can be streamed on-demand from online sources via an API (e.g., the Bloomberg financial data stream).\n",
    "\n",
    "> Data can be automatically generated by physical apparatus, such as scientific lab equipment attached to computers.\n",
    "\n",
    "> Data can be generated by computer software, such as logs from a webserver or classifications produced by a machine learning algorithm.\n",
    "\n",
    "> Data can be manually entered into a spreadsheet or text file by a human.\n",
    "\n",
    "Considerations for Data Acquistion: \n",
    "\n",
    "> Keeping track of provenance (where each piece of data comes from and whether it is still up-to-date):  \n",
    "\n",
    ">> (It is important to accurately track provenance, since data often needs to be re-acquired in the future to run updated experiments.  Re-acquisition can occur either when the original data sources get updated or when researchers want to test alternate hypotheses.  Also, provenance can enable downstream analysis errors to be traced back to the original data sources.)\n",
    "\n",
    "> Data management: \n",
    "\n",
    ">> Programmers must assign names to data files that they create or download and then organize those files into directories.  When they create or download new versions of those files, they must make sure to assign proper filenames to all versions and keep track of their differences.\n",
    "\n",
    " >Storage: \n",
    ">>Sometimes there is so much data that it cannot fit on a single hard drive, so it must be stored on remote servers.  However, anecdotes and empirical studies indicate that a significant amount of data analysis is still done on desktop machines with data sets that fit on modern hard drives (i.e., less than a terabyte)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. PARSE THE DATA\n",
    "\n",
    "- Read any documentation provided with the data\n",
    "- Perform exploratory data analysis\n",
    "- Verify the quality of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. MINE THE DATA\n",
    "\n",
    "- Determine sampling methodology and sample data\n",
    "- Format, clean, slice, and combine data in Python\n",
    "- Create necessary dervied columns from the data (new data)\n",
    "\n",
    "\n",
    "Reformat and clean data: \n",
    "\n",
    "> Raw data is probably not in a convenient format for a data scientist to run a particular analysis, often due to the simple reason that it was formatted by somebody else without that programmer's analysis in mind.  \n",
    "\n",
    "> Raw data often contains semantic errors, missing entries, or inconsistent formatting, so it needs to be \"cleaned\" prior to analysis.\n",
    "\n",
    "> In sum, data munging and organization are human productivity bottlenecks that must be overcome before actual substantive analysis can be done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing, executing, and refining computer programs to analyze and obtain insights from data.  \n",
    "\n",
    "Repeated iteration cycle of editing scripts, executing to produce output files, inspecting the output files to gain insights and discover mistakes, debugging, and re-editing.\n",
    "\n",
    "Considerations:\n",
    "\n",
    "> Absolute running times: Scripts might take a long time to terminate, either due to large amounts of data being processed or the algorithms being slow, which could itself be due to asymptotic \"Big-O\" slowness and/or the implementations being slow.\n",
    "\n",
    "> Incremental running times: Scripts might take a long time to terminate after minor incremental code edits done while iterating on analyses, which wastes time re-computing almost the same results as previous runs.\n",
    "\n",
    "> Crashes from errors: Scripts might crash prematurely due to errors in either the code or inconsistencies in data sets. Programmers often need to endure several rounds of debugging and fixing banal bugs such as data parsing errors before their scripts can terminate with useful results.\n",
    "\n",
    "> File and metadata management is another challenge in the analysis phase. Repeatedly editing and executing scripts while iterating on experiments causes the production of numerous output files, such as intermediate data, textual reports, tables, and graphical visualizations.  \n",
    "\n",
    "\n",
    "Lastly, data scientists do not write code in a vacuum: As they iterate on their scripts, they often consult resources such as documentation websites, API usage examples, sample code snippets from online forums, PDF documents of related research papers, and relevant code obtained from colleagues.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. REFINE THE DATA  (EXPLORE THE DATA)\n",
    "\n",
    "- Identify trends and outliers\n",
    "- Apply descriptive and inferential statistics\n",
    "- Document and transform data\n",
    "\n",
    "> Skills: Get to know data, develop hypotheses, patterns? anomalies?\n",
    "\n",
    "> Tools: matplotlib, numpy, scipy, pandas, (spark, pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. BUILD A DATA MODEL\n",
    "\n",
    "- Select appropriate model\n",
    "- Build model\n",
    "- Evaluate and refine model\n",
    "\n",
    "> Skills: regression, machine learning, validation, big data\n",
    "\n",
    "> Tools: scikits learn, pandas (spark, pyspark, MLlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists frequently alternate between the analysis and reflection phases while they work\n",
    "\n",
    ">Take notes\n",
    "\n",
    ">> Since notes are a form of data, the usual data management problems arise in notetaking, most notably how to organize notes and link them with the context in which they were originally written.\n",
    "\n",
    "> Hold meetings\n",
    "\n",
    "> Make comparisons and explore alternatives: \n",
    "\n",
    ">>The reflection activities that tie most closely with the analysis phase are making comparisons between output variants and then exploring alternatives by adjusting script code and/or execution parameters.\n",
    "\n",
    "> Much of the analysis process is trial-and-error: \n",
    ">> Run tests, graph the output, rerun them, graph the output, etc.  The scientists rely heavily on graphs --- they graph the output and distributions of their tests, they graph the sequenced genomes next to other, existing sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. PRESENT RESULTS (COMMUNICATE THE DATA)\n",
    "\n",
    "- Summarize with narrative, storytelling techniques\n",
    "- Present limitations and assumptions of your analysis\n",
    "- Identify follow up issues for future analysis\n",
    "\n",
    "> Skills: presentation, speaking, visuals, writing\n",
    "\n",
    "> Tools: matplotlib, adobe illustrator, powerpoint/keynote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. DEPLOY AND VALIDATE (IMPLEMENTATION)\n",
    "\n",
    "- Write unit tests and documentation\n",
    "- Deploy stable production-ready code\n",
    "- Retrain and validate models over time\n",
    "\n",
    "\n",
    "> Skills: product management, communication, IO psychology, CS, politics.\n",
    "Stage 6 is hugely important. Data Scientists who don’t carry their work all the way through to realize it’s full impact are ultimately just ineffective consultants. When companies’ don’t understand how data science really helps their organization it’s (often) because they have data scientists that stop at Stage 5. Or really bad organizational structure that has siloed DS.\n",
    "\n",
    " > You probably can’t implement it on your own. But that doesn’t mean it isn’t your job to communicate, collaborate, and convince until it is implemented. Depending on what you found, it might require people to change the way they make decisions, or a product squad to alter their strategy and AB test a new feature, your ML engineer (if it isn’t you) to prototype a new feature based on your insight. If you are the ML engineer it might require getting other engineers to change a data pipeline in production to support your new model. Champion your work—no one else knows how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. TEST AND QUANTIFY YOUR IMPACT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> Skills: all of the above.\n",
    "\n",
    "> Did Stage 6 work? Was this project worthwhile? This is not a subjective question. Who better to answer this question than you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISSEMINATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final phase is disseminating results.\n",
    "\n",
    "> Commonly in the form of written reports such as internal memos, slideshow presentations, business/policy white papers, or academic research publications. \n",
    "\n",
    "\n",
    "Beyond presenting results in written form, some data scientists also want to distribute their software so that colleagues can reproduce their experiments or play with their prototype systems.  \n",
    "\n",
    "Considerations:\n",
    "> Sometimes it is difficult to reproduce the results of one's own experiments a few months or years in the future, since one's own operating system and software inevitably get upgraded in some incompatible manner such that the original code no longer runs.  \n",
    "\n",
    "Lastly, data scientists often collaborate with colleagues by sending them partial results to receive feedback and fresh ideas. A variety of logistical and communication challenges arise in collaborations centered on code and data sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent practice (5 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Given your capstone project, look at each step and clearly lay out how you would approach each step in the data science workflow and tailor it to your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDITIONAL RESOURCES\n",
    "\n",
    "- [Data Science Workflow: Overview and Challenges](http://cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext)\n",
    "- [Quora: what is the workflow or process of a data scientist](https://www.quora.com/What-is-the-workflow-or-process-of-a-data-scientist)\n",
    "\n",
    "- [BinaryEdge: Data Science Workflow](http://blog.binaryedge.io/2015/09/08/the-data-science-workflow/)\n",
    "- [Guerilla Analytics: Data Science Workflow: A Reality Check](https://guerrilla-analytics.net/2015/02/20/data-science-workflows-a-reality-check/)\n",
    "- [Data Science 101: THE GOAL IS DATA PRODUCTS: NOW HOW DO WE GET THERE?](http://101.datascience.community/tag/workflow/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {
    "height": "408px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
